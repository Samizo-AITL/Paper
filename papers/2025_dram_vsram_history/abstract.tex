% ---------- Abstract ----------
\begin{abstract}
In 1998, a 3rd-generation 64-Mbit DRAM was transferred and ramped on a 0.25-\si{\micro\meter} process using short-cycle feedback (SCF) and a parallel introduction of production and margin lots. The initial yield was $\sim$65\%, dominated by retention-related failures under pause-refresh (Bin-5) and disturb-refresh (Bin-6) tests. Process tracing indicated cumulative plasma damage from resist ashing after WSA-ET and multiple LDD steps as a primary root cause. By shifting resist stripping to wet processes and strengthening body back-bias, the yield improved to $\sim$80\% and passed long-term reliability. 

Building on that platform, a pseudo-SRAM (VSRAM) was mass-produced in 2001 by adding internal refresh control and extending the operating guarantee from 80 to 90~°C for mobile use. Although the initial yield was only $\sim$30\%, production started as a rational business decision to secure early market entry, and yield improved to 80--90\% with continued countermeasures. At the 0.18-\si{\micro\meter} node, however, trench-capacitor VSRAM failed to meet retention requirements at 90~°C due to larger junction leakage, marking the practical end of 1T-1C pseudo-SRAM evolution. We position this case as a practical ramp-up design framework and an instructive archive linking historical process learning to modern reliability phenomena.
\end{abstract}
