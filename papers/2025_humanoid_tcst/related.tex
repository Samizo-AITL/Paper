\section{Related Work}
Classical humanoid control has been dominated by proportional–integral–derivative (PID) loops,
which provide joint-level stabilization and trajectory tracking with simplicity and robustness.
Boston Dynamics’ \textit{Atlas} demonstrates highly dynamic behaviors such as jumping and flipping,
achieved through advanced mechanical design and optimized low-level controllers.
In contrast, Tesla’s \textit{Optimus} prioritizes scalable production for industrial assistance,
emphasizing simplified locomotion and manipulation.

Beyond PID control, state-space methods such as the linear quadratic regulator (LQR)
and linear quadratic Gaussian (LQG) have been applied to multi-input multi-output humanoid systems,
enabling systematic stability analysis and optimal feedback design.
Recent research also explores reinforcement learning for adaptive control,
although training complexity and safety concerns remain significant challenges.

Integration of symbolic reasoning with classical control has received limited attention.
While finite state machines (FSMs) provide interpretable supervisory logic,
their combination with advanced learning models is still emerging.
In particular, the use of large language models (LLMs) within humanoid control
remains underexplored. This work advances the field by embedding LLMs
into a hierarchical control loop: the LLM layer generates goals,
interprets anomalies, and supports human–robot interaction,
while stability and safety are ensured by PID and state-space controllers.
